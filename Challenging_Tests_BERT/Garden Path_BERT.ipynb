{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e9f6882",
   "metadata": {},
   "source": [
    "### Konstantina Andronikou "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dacdf3f",
   "metadata": {},
   "source": [
    "## This notebook implements a Garden Path sentence "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20238d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp_models.pretrained import load_predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f167f417",
   "metadata": {},
   "outputs": [],
   "source": [
    "import checklist\n",
    "import logging \n",
    "import csv\n",
    "from checklist.editor import Editor\n",
    "from checklist.perturb import Perturb\n",
    "from checklist.test_types import MFT, INV, DIR\n",
    "from checklist.expect import Expect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5388d331",
   "metadata": {},
   "outputs": [],
   "source": [
    "from checklist.pred_wrapper import PredictorWrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e63784e",
   "metadata": {},
   "source": [
    "### Syntactic Ambiguity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "354c85f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "logging.getLogger('allennlp.common.params').disabled = True \n",
    "logging.getLogger('allennlp.nn.initializers').disabled = True \n",
    "logging.getLogger('allennlp.modules.token_embedders.embedding').disabled = True \n",
    "logging.getLogger('urllib3.connectionpool').disabled = True \n",
    "logging.getLogger('allennlp.common.plugins').disabled = True \n",
    "logging.getLogger('allennlp.common.model_card').disabled = True \n",
    "logging.getLogger('allennlp.models.archival').disabled = True \n",
    "logging.getLogger('allennlp.data.vocabulary').disabled = True \n",
    "logging.getLogger('cached_path').disabled = True\n",
    "srl_predictor = load_predictor('structured-prediction-srl-bert')\n",
    "output = srl_predictor.predict(\"Mary gave the child the dog bit a Band-Aid.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a34a711",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_srl(data):\n",
    "    \n",
    "    pred = []\n",
    "    for d in data:\n",
    "        pred.append(srl_predictor.predict(d))\n",
    "    return pred\n",
    "\n",
    "predict_and_conf = PredictorWrapper.wrap_predict(predict_srl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af039ac4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "d = [\"Mary gave the child the dog bit a Band-Aid.\"]\n",
    "pred = predict_and_conf(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cbbf6695",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_srl(x, pred, conf, label=None, meta=None):\n",
    "    \n",
    "    return pred['verbs'][0]['description']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eabfe121",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_arg(pred, arg_target='ARG1'):\n",
    "    predicate_arguments = pred['verbs'][0]\n",
    "    words = pred['words']\n",
    "    tags = predicate_arguments['tags']\n",
    "    \n",
    "    arg_list = []\n",
    "    for t, w in zip(tags, words):\n",
    "        arg = t\n",
    "        if '-' in t:\n",
    "            arg = t.split('-')[1]\n",
    "        if arg == arg_target:\n",
    "            arg_list.append(w)\n",
    "    arg_set = set(arg_list)\n",
    "    return arg_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7bd6a9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def found_arg1_people(x, pred, conf, label=None, meta=None):\n",
    "    \n",
    "    # people should be recognized as arg1\n",
    "\n",
    "    people = set([meta['first_name1']])\n",
    "    arg_1 = get_arg(pred, arg_target='ARG1')\n",
    "\n",
    "    if arg_1 == people:\n",
    "        pass_ = True\n",
    "    else:\n",
    "        pass_ = False\n",
    "    return pass_\n",
    "\n",
    "\n",
    "expect_arg1 = Expect.single(found_arg1_people)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4cf0d909",
   "metadata": {},
   "outputs": [],
   "source": [
    "editor = Editor()\n",
    "\n",
    "# create examples\n",
    "t = editor.template('Mary gave {first_name1} the dog bit a Band-Aid.', meta=True, nsamples=1000, remove_duplicates = True)\n",
    "\n",
    "#print(type(t))\n",
    "\n",
    "# for k, v in t.items():\n",
    "#     print(k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9f1fdaa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Dataset_BERT/Garden_Path.txt', 'w') as f:\n",
    "    print(t.data, file = f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "72618b90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 1000 examples\n",
      "Test cases:      1000\n",
      "Fails (rate):    1000 (100.0%)\n",
      "\n",
      "Example fails:\n",
      "[ARG0: Mary] [V: gave] [ARG2: Frances] [ARG1: the dog] bit a Band - Aid .\n",
      "----\n",
      "[ARG0: Mary] [V: gave] [ARG2: John] [ARG1: the dog bit a Band - Aid] .\n",
      "----\n",
      "[ARG0: Mary] [V: gave] [ARG2: Caroline] [ARG1: the] dog bit a Band - Aid .\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "# initialize a rest object\n",
    "test = MFT(**t, name = 'detect_arg1_name_default_position', expect=expect_arg1)\n",
    "output = test.run(predict_and_conf)\n",
    "test.summary(format_example_fn=format_srl)\n",
    "i = test.results['preds']\n",
    "expect_for_i = test.results['expect_results']\n",
    "with open ('Output_BERT/Garden_path_BERT.csv','w') as f:\n",
    "    writer = csv.writer(f)\n",
    "    for result, exp in zip(i, expect_for_i):\n",
    "        \n",
    "        case = result['words']\n",
    "        exp = exp\n",
    "        writer.writerow([case, exp])\n",
    "#         print(case,exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1c378008",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the following piece of code was adapted from https://howtodoinjava.com/examples/python-print-to-file/\n",
    "import sys\n",
    "original_stdout = sys.stdout  \n",
    " \n",
    "with open('Predictions_BERT/false_sentences_Garden_path_BERT.txt', 'a') as f:\n",
    "    sys.stdout = f \n",
    "    print(test.summary(format_example_fn=format_srl), file = f)\n",
    "    # Reset the standard output\n",
    "    sys.stdout = original_stdout "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ce0a27",
   "metadata": {},
   "source": [
    "### Lexical Ambiguity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5ea260d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_srl(data):\n",
    "    \n",
    "    pred = []\n",
    "    for d in data:\n",
    "        pred.append(srl_predictor.predict(d))\n",
    "    return pred\n",
    "\n",
    "predict_and_conf = PredictorWrapper.wrap_predict(predict_srl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "650a7165",
   "metadata": {},
   "outputs": [],
   "source": [
    "def found_arg1_people(x, pred, conf, label=None, meta=None):\n",
    "    \n",
    "    # people should be recognized as arg1\n",
    "\n",
    "    people = set([meta['animal']])\n",
    "    arg_1 = get_arg(pred, arg_target='ARG1')\n",
    "\n",
    "    if arg_1 == people:\n",
    "        pass_ = True\n",
    "    else:\n",
    "        pass_ = False\n",
    "    return pass_\n",
    "\n",
    "\n",
    "expect_arg1 = Expect.single(found_arg1_people)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e8c8d760",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "d = [\"The {animal} raced past the barn fell.\"]\n",
    "animal = ['horse', 'dog', 'cat', 'rabbit']\n",
    "pred = predict_and_conf(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c7a8ecbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "editor = Editor()\n",
    "\n",
    "# create examples\n",
    "t = editor.template('The {animal} raced past the barn fell.', animal=animal, meta=True, nsamples=4, remove_duplicates = True)\n",
    "#print(type(t))\n",
    "\n",
    "# for k, v in t.items():\n",
    "#     print(k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b13493b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Dataset_BiLSTM/Garden_Path_Lexical_BERT .txt', 'w') as f:\n",
    "    print(t.data, file = f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3c79f49f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 4 examples\n",
      "Test cases:      4\n",
      "Fails (rate):    4 (100.0%)\n",
      "\n",
      "Example fails:\n",
      "[ARG0: The horse] [V: raced] [ARGM-DIR: past the barn] fell .\n",
      "----\n",
      "[ARG0: The horse] [V: raced] [ARGM-DIR: past the barn] fell .\n",
      "----\n",
      "[ARG0: The cat] [V: raced] [ARGM-DIR: past the barn] fell .\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "# initialize a rest object\n",
    "test = MFT(**t, name = 'detect_arg1_name_default_position', expect=expect_arg1)\n",
    "output = test.run(predict_and_conf)\n",
    "test.summary(format_example_fn=format_srl)\n",
    "i = test.results['preds']\n",
    "expect_for_i = test.results['expect_results']\n",
    "with open ('Output_BiLSTM/Garden_path_Lexical_BERT.csv','w') as f:\n",
    "    writer = csv.writer(f)\n",
    "    for result, exp in zip(i, expect_for_i):\n",
    "        \n",
    "        case = result['words']\n",
    "        exp = exp\n",
    "        writer.writerow([case, exp])\n",
    "        #print(case,exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5311d817",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the following piece of code was adapted from https://howtodoinjava.com/examples/python-print-to-file/\n",
    "import sys\n",
    "original_stdout = sys.stdout  \n",
    " \n",
    "with open('Predictions_BiLSTM/false_sentences_Garden_path_lexical_BERT.txt', 'a') as f:\n",
    "    sys.stdout = f \n",
    "    print(test.summary(format_example_fn=format_srl), file = f)\n",
    "    # Reset the standard output\n",
    "    sys.stdout = original_stdout "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
